{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "mpdDNHRNDJTa",
    "outputId": "06ab3e05-dd9d-4e1f-a760-42d83c0aa209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ErKkZgb_DOtU",
    "outputId": "addca156-fe94-4a65-bc78-d385f9835485"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Dropout, Bidirectional, LSTM, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mido import Message, MidiFile, MidiTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFDbzzcfFLx9"
   },
   "outputs": [],
   "source": [
    "datapath = 'drive/My Drive/NN/Capstone/classical_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nPwF0c50FTdr",
    "outputId": "0b176be4-450d-4b90-ff6c-e758c0cd9609"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001,)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_arr = np.load(datapath + '/songs_arr.npy', allow_pickle=True)\n",
    "songs_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkazrzwu6X5h"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "n_features = 3\n",
    "batch_size = 128\n",
    "latent_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJwCbM2zM10F"
   },
   "outputs": [],
   "source": [
    "## Generate sequences\n",
    "notes_seq_in = []\n",
    "\n",
    "for song in songs_arr:\n",
    "  idx = 0\n",
    "  while idx < song.shape[0] - seq_length - 1:\n",
    "    seq = song[idx: idx + seq_length, :]\n",
    "    notes_seq_in.append(seq)\n",
    "    idx = idx + seq_length//2\n",
    "    \n",
    "notes_seq_in_arr = np.array(notes_seq_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd3oQYXTNIyI"
   },
   "outputs": [],
   "source": [
    "num_batches = int(notes_seq_in_arr.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hCyYeCuPgI3N",
    "outputId": "e8c94e0c-607e-4c26-e957-cf27e1bf9c79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNveCSB3TG_T"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "\n",
    "*   Convert data from MIDI number to frequency\n",
    "*   Normalize Notes from -1 to 1\n",
    "*   Convert Intensity to a binary variable\n",
    "*   Normalize Time from -1 to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsUxLwjnqCll"
   },
   "outputs": [],
   "source": [
    "notes_seq_in_arr_norm = np.zeros(notes_seq_in_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nf53T6JIS58Y"
   },
   "outputs": [],
   "source": [
    "notes = notes_seq_in_arr[:, :, 0]\n",
    "notes_to_freq = np.exp2((notes - 69)/12) * 440\n",
    "freq_max = np.max(notes_to_freq)\n",
    "freq_min = np.min(notes_to_freq)\n",
    "notes_to_freq_norm = (notes_to_freq - freq_min) / (freq_max - freq_min)\n",
    "notes_seq_in_arr_norm[:, :, 0] = notes_to_freq_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QXvagf0DxsRR",
    "outputId": "291d6274-6ccd-4f7c-9330-5b4c614de96e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.175798915643707, 12543.853951415975)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_min, freq_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jezmGoVvec1X",
    "outputId": "7775ccb4-bfe6-4d3f-d708-ac6b4f5c3dfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182041"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VmPL6f1mnNO"
   },
   "outputs": [],
   "source": [
    "intensity = notes_seq_in_arr[:, :, 1]\n",
    "intensity_norm = intensity / 127\n",
    "notes_seq_in_arr_norm[:, :, 1] = intensity_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89COJt7-m6fV"
   },
   "outputs": [],
   "source": [
    "time = notes_seq_in_arr[:, :, 2]\n",
    "time_max = np.max(time)\n",
    "time_min = np.min(time)\n",
    "time_norm = (time - time_min) / (time_max - time_min)\n",
    "notes_seq_in_arr_norm[:, :, 2] = time_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2_mixi9Mh2J"
   },
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2NHxyKPFjKd"
   },
   "outputs": [],
   "source": [
    "#Generator\n",
    "def Generator(noise_dim, drop_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim = noise_dim))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense((seq_length*n_features),activation='sigmoid'))\n",
    "    model.add(Reshape((seq_length, n_features)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHl-qsU7PGGF"
   },
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "def Discriminator(input_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(500, input_shape=input_size))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "5tO9vGPhPpYj",
    "outputId": "94f662bb-49eb-4af7-cba3-d6a84e323f91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator((100, 3))\n",
    "disc.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "vU_n7481RZbr",
    "outputId": "f9b05309-55fd-4d02-ac30-64e7fdafbba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(10, 0.2)\n",
    "gen.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Jjof9wVRrut"
   },
   "outputs": [],
   "source": [
    "combined_gan = Sequential()\n",
    "combined_gan.add(gen)\n",
    "disc.trainable = False\n",
    "combined_gan.add(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOoqLlW3StXJ"
   },
   "outputs": [],
   "source": [
    "combined_gan.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e80AGDIjSwy8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Adversarial ground truths\n",
    "real = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "# Store losses for analysis\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(10):\n",
    "  print(\"Epoch number \" + str(epoch))\n",
    "  \n",
    "  batch_idx = np.random.randint(0, num_batches-1)\n",
    "  \n",
    "  # Generate random noise for generator\n",
    "  noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    \n",
    "  # Generate noisy sequences through generator\n",
    "  gen_seqs = gen.predict(noise)\n",
    "    \n",
    "  # Train on real sequences\n",
    "  d_loss_real = disc.train_on_batch(notes_seq_in_arr[batch_idx*batch_size:(batch_idx+1)*batch_size, :, :], real)\n",
    "  \n",
    "  # Train on noise\n",
    "  d_loss_fake = disc.train_on_batch(gen_seqs, fake)\n",
    "  \n",
    "  print(d_loss_fake)\n",
    "  \n",
    "  # Compute total loss function\n",
    "  d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "  # Compute generator loss keeping discriminator constant\n",
    "  noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "  g_loss = combined_gan.train_on_batch(noise, real)\n",
    "  \n",
    "  \n",
    "  noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "  gen_seqs = gen.predict(noise)\n",
    "  \n",
    "  test_batch_idx = np.random.randint(0, num_batches-1)\n",
    "  d_loss_real_test = disc.test_on_batch(notes_seq_in_arr[test_batch_idx*batch_size:(test_batch_idx+1)*batch_size, :, :], real)\n",
    "  \n",
    "  # Train on noise\n",
    "  d_loss_fake_test = disc.test_on_batch(gen_seqs, fake)\n",
    "  \n",
    "  d_loss_test = 0.5 * np.add(d_loss_real_test, d_loss_fake_test)\n",
    "  \n",
    "  noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "  g_loss_test = combined_gan.test_on_batch(noise, real)\n",
    "  \n",
    "  disc_losses.append(d_loss_test[0])\n",
    "  gen_losses.append(g_loss)\n",
    "  \n",
    "  #print(\"Discriminator Loss is \" + str(disc_losses[-1]))\n",
    "  #print(\"Generator Loss is \" + str(gen_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "yuOIWZm9-zR6",
    "outputId": "7ba2adfd-500d-406b-c3b1-4cc651fc1c82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyzkHgZlZzjkIzMxy7n8AYIw1qarox1MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(disc_losses, c='red')\n",
    "plt.plot(gen_losses, c='blue')\n",
    "plt.title(\"GAN Loss per Epoch\")\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHc9izSdZlgi"
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "gen_seqs = gen.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RF78X605bqot"
   },
   "outputs": [],
   "source": [
    "mid = MidiFile()\n",
    "track = MidiTrack()\n",
    "mid.tracks.append(track)\n",
    "\n",
    "for i in range(0, 100):\n",
    "  track.append(Message('note_on', note=int(gen_seqs[i, 0]), time=int(gen_seqs[i, 2])))\n",
    "\n",
    "mid.save(datapath + 'new_song.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvQ6-PhSGsLy"
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    \n",
    "    def __init__(self, rows):\n",
    "        self.seq_length = rows\n",
    "        self.seq_shape = (self.seq_length, 3)\n",
    "        self.latent_dim = 100\n",
    "        self.disc_loss_real = []\n",
    "        self.disc_loss_fake = []\n",
    "        self.gen_loss =[]\n",
    "        \n",
    "        #optimizer_disc = Adam(0.00003)\n",
    "        #optimizer_gen = Adam(0.0003)\n",
    "        optimizer_disc = Adam(0.0001)\n",
    "        optimizer_gen = Adam(0.001)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer_disc, metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator()\n",
    "        self.generator = self.build_generator_lstm()\n",
    "        \n",
    "        # The generator takes noise as input and generates note sequences\n",
    "        #z = Input(shape=(self.latent_dim, ))\n",
    "        z = Input(shape=(self.latent_dim,1))\n",
    "        generated_seq = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(generated_seq)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_gen)\n",
    "\n",
    "        \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(512, input_shape=self.seq_shape))\n",
    "        #model.add(Bidirectional(LSTM(512)))\n",
    "        #model.add(Dense(512))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        seq = Input(shape=self.seq_shape)\n",
    "        validity = model(seq)\n",
    "\n",
    "        return Model(seq, validity)\n",
    "      \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.seq_shape), activation='sigmoid'))\n",
    "        model.add(Reshape(self.seq_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        seq = model(noise)\n",
    "\n",
    "        return Model(noise, seq)\n",
    "      \n",
    "      \n",
    "    def build_generator_lstm(self):\n",
    "      \n",
    "      model = Sequential()\n",
    "      model.add(LSTM(256, input_shape=(self.latent_dim, 1), return_sequences=True))\n",
    "      model.add(Dropout(0.3))\n",
    "      model.add(LeakyReLU(alpha=0.2))\n",
    "      #model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "      model.add(TimeDistributed(Dense(512)))\n",
    "      model.add(TimeDistributed(LeakyReLU(alpha=0.2)))\n",
    "      model.add(TimeDistributed(BatchNormalization(momentum=0.8)))\n",
    "      model.add(TimeDistributed(Dense(3)))\n",
    "      \n",
    "      model.summary()\n",
    "        \n",
    "      noise = Input(shape=(self.latent_dim, 1))\n",
    "      seq = model(noise)\n",
    "\n",
    "      return Model(noise, seq)\n",
    "      \n",
    "      \n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load and convert the data\n",
    "        X_train = notes_seq_in_arr_norm\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        seqs_list = []\n",
    "        \n",
    "        # Training the model\n",
    "        for epoch in range(epochs):\n",
    "          \n",
    "#             if epoch % 5 == 0:\n",
    "#               self.discriminator.trainable = True\n",
    "#             else:\n",
    "#               self.discriminator.trainable = False\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Select a random batch of note sequences\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            #print(idx)\n",
    "            real_seqs = X_train[idx]\n",
    "            #print(\"shape: \", real_seqs.shape)\n",
    "            #noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n",
    "            \n",
    "            #print(noise.shape)\n",
    "      \n",
    "            # Generate a batch of new note sequences\n",
    "            gen_seqs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #  Training the Generator\n",
    "            #noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n",
    "            \n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, real)\n",
    "            \n",
    "            if d_loss[0] < 0.7 * g_loss:\n",
    "              self.discriminator.trainable = False\n",
    "              self.generator.trainable = True\n",
    "            elif g_loss < 0.7 * d_loss[0]:\n",
    "              self.discriminator.trainable = True\n",
    "              self.generator.trainable = False\n",
    "            else:\n",
    "              self.discriminator.trainable = True\n",
    "              self.generator.trainable = True\n",
    "\n",
    "            # Print the progress and save into loss lists\n",
    "            if epoch % sample_interval == 0:\n",
    "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "              #self.generator.save(datapath + \"final_model\"+str(epoch)+\".hdf5\")\n",
    "              #noise = np.random.normal(0, 1, (1, 100, 1)) \n",
    "              #seqs_list.append(self.generator.predict(noise))\n",
    "            \n",
    "            #if epoch % 5 == 0:\n",
    "            self.disc_loss_real.append(d_loss[0])\n",
    "            self.disc_loss_fake.append(d_loss[1])\n",
    "            self.gen_loss.append(g_loss)\n",
    "        \n",
    "        #self.generate(notes)\n",
    "        self.plot_loss()\n",
    "        return seqs_list\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.disc_loss_real, c='dodgerblue')\n",
    "        plt.plot(self.disc_loss_fake, c='orange')\n",
    "        plt.plot(self.gen_loss, c='green')\n",
    "        plt.title(\"GAN Loss per Epoch\")\n",
    "        plt.legend(['Discriminator(Real)', 'Discriminator(Fake)', 'Generator'], loc=\"best\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig(datapath + 'loss_epoch_lstm.png', transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vf5p-g667J9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10292
    },
    "colab_type": "code",
    "id": "qZ8CcqmtG2rw",
    "outputId": "c473e5c5-00ee-42e1-c243-5093960cc274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 512)               1056768   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,188,353\n",
      "Trainable params: 1,188,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 100, 3)            0         \n",
      "=================================================================\n",
      "Total params: 997,420\n",
      "Trainable params: 993,836\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.701868, acc.: 33.20%] [G loss: 0.693907]\n",
      "5 [D loss: 0.649026, acc.: 97.27%] [G loss: 0.778669]\n",
      "10 [D loss: 0.592083, acc.: 98.83%] [G loss: 0.925906]\n",
      "15 [D loss: 0.453583, acc.: 99.61%] [G loss: 1.608199]\n",
      "20 [D loss: 0.087623, acc.: 100.00%] [G loss: 10.686011]\n",
      "25 [D loss: 0.502659, acc.: 91.02%] [G loss: 10.676782]\n",
      "30 [D loss: 0.490210, acc.: 82.81%] [G loss: 9.220753]\n",
      "35 [D loss: 0.469845, acc.: 80.86%] [G loss: 7.199968]\n",
      "40 [D loss: 0.470490, acc.: 77.34%] [G loss: 5.904325]\n",
      "45 [D loss: 0.445944, acc.: 80.08%] [G loss: 5.435566]\n",
      "50 [D loss: 0.462598, acc.: 78.12%] [G loss: 6.340213]\n",
      "55 [D loss: 0.487375, acc.: 75.78%] [G loss: 5.935518]\n",
      "60 [D loss: 0.453437, acc.: 77.73%] [G loss: 6.023584]\n",
      "65 [D loss: 0.740699, acc.: 75.78%] [G loss: 4.185201]\n",
      "70 [D loss: 0.494976, acc.: 73.44%] [G loss: 2.347134]\n",
      "75 [D loss: 0.495910, acc.: 73.44%] [G loss: 2.106502]\n",
      "80 [D loss: 0.426146, acc.: 80.47%] [G loss: 2.547625]\n",
      "85 [D loss: 0.446033, acc.: 76.56%] [G loss: 2.367205]\n",
      "90 [D loss: 0.510683, acc.: 70.70%] [G loss: 2.840663]\n",
      "95 [D loss: 0.440360, acc.: 77.34%] [G loss: 3.162788]\n",
      "100 [D loss: 0.523003, acc.: 65.23%] [G loss: 3.085764]\n",
      "105 [D loss: 0.314361, acc.: 86.33%] [G loss: 3.253148]\n",
      "110 [D loss: 0.216134, acc.: 92.19%] [G loss: 3.312051]\n",
      "115 [D loss: 0.775192, acc.: 72.27%] [G loss: 3.664958]\n",
      "120 [D loss: 0.240401, acc.: 92.58%] [G loss: 3.677961]\n",
      "125 [D loss: 0.202001, acc.: 93.36%] [G loss: 3.768506]\n",
      "130 [D loss: 0.220249, acc.: 91.02%] [G loss: 3.786158]\n",
      "135 [D loss: 0.281403, acc.: 85.16%] [G loss: 4.187086]\n",
      "140 [D loss: 0.246103, acc.: 88.28%] [G loss: 3.890351]\n",
      "145 [D loss: 0.685677, acc.: 51.95%] [G loss: 4.055238]\n",
      "150 [D loss: 0.542889, acc.: 68.75%] [G loss: 4.233346]\n",
      "155 [D loss: 0.523529, acc.: 72.66%] [G loss: 4.424982]\n",
      "160 [D loss: 0.540757, acc.: 70.70%] [G loss: 4.591866]\n",
      "165 [D loss: 0.511745, acc.: 73.05%] [G loss: 4.484515]\n",
      "170 [D loss: 0.532509, acc.: 69.92%] [G loss: 4.682637]\n",
      "175 [D loss: 0.536797, acc.: 66.80%] [G loss: 4.573725]\n",
      "180 [D loss: 0.545357, acc.: 67.19%] [G loss: 4.465278]\n",
      "185 [D loss: 0.474623, acc.: 77.73%] [G loss: 4.673085]\n",
      "190 [D loss: 0.542005, acc.: 71.88%] [G loss: 4.237544]\n",
      "195 [D loss: 0.477879, acc.: 73.83%] [G loss: 4.729451]\n",
      "200 [D loss: 0.461660, acc.: 75.00%] [G loss: 4.724716]\n",
      "205 [D loss: 0.452659, acc.: 76.95%] [G loss: 4.520750]\n",
      "210 [D loss: 0.540007, acc.: 71.88%] [G loss: 4.447955]\n",
      "215 [D loss: 0.497392, acc.: 73.44%] [G loss: 5.085313]\n",
      "220 [D loss: 0.453934, acc.: 77.73%] [G loss: 4.512703]\n",
      "225 [D loss: 0.524356, acc.: 70.31%] [G loss: 4.851469]\n",
      "230 [D loss: 0.503967, acc.: 74.61%] [G loss: 4.188063]\n",
      "235 [D loss: 0.493377, acc.: 73.44%] [G loss: 4.502163]\n",
      "240 [D loss: 0.534132, acc.: 68.36%] [G loss: 4.945182]\n",
      "245 [D loss: 0.441142, acc.: 76.56%] [G loss: 5.070783]\n",
      "250 [D loss: 1.401953, acc.: 71.88%] [G loss: 5.892589]\n",
      "255 [D loss: 0.487254, acc.: 75.78%] [G loss: 4.220286]\n",
      "260 [D loss: 0.452408, acc.: 77.34%] [G loss: 3.817207]\n",
      "265 [D loss: 0.441715, acc.: 79.69%] [G loss: 3.770058]\n",
      "270 [D loss: 0.460185, acc.: 76.56%] [G loss: 3.680988]\n",
      "275 [D loss: 0.478245, acc.: 76.56%] [G loss: 3.898686]\n",
      "280 [D loss: 0.473613, acc.: 75.78%] [G loss: 3.786059]\n",
      "285 [D loss: 0.490775, acc.: 73.44%] [G loss: 3.582904]\n",
      "290 [D loss: 0.484243, acc.: 72.66%] [G loss: 4.098914]\n",
      "295 [D loss: 0.491171, acc.: 73.05%] [G loss: 3.914717]\n",
      "300 [D loss: 0.521450, acc.: 71.09%] [G loss: 3.748720]\n",
      "305 [D loss: 0.471326, acc.: 73.83%] [G loss: 3.830489]\n",
      "310 [D loss: 0.466080, acc.: 75.39%] [G loss: 3.602274]\n",
      "315 [D loss: 0.481056, acc.: 73.83%] [G loss: 4.204679]\n",
      "320 [D loss: 0.431566, acc.: 78.91%] [G loss: 3.918187]\n",
      "325 [D loss: 0.515151, acc.: 72.66%] [G loss: 4.227522]\n",
      "330 [D loss: 0.467334, acc.: 74.61%] [G loss: 3.685851]\n",
      "335 [D loss: 0.476789, acc.: 75.39%] [G loss: 4.051635]\n",
      "340 [D loss: 0.493047, acc.: 71.88%] [G loss: 4.437347]\n",
      "345 [D loss: 0.478572, acc.: 73.83%] [G loss: 4.204971]\n",
      "350 [D loss: 0.486664, acc.: 73.44%] [G loss: 4.296899]\n",
      "355 [D loss: 0.524066, acc.: 69.53%] [G loss: 4.239006]\n",
      "360 [D loss: 0.503134, acc.: 69.92%] [G loss: 4.433209]\n",
      "365 [D loss: 0.472417, acc.: 73.44%] [G loss: 4.129537]\n",
      "370 [D loss: 0.493168, acc.: 71.48%] [G loss: 4.402984]\n",
      "375 [D loss: 0.534335, acc.: 67.97%] [G loss: 4.228795]\n",
      "380 [D loss: 0.514188, acc.: 69.14%] [G loss: 4.268276]\n",
      "385 [D loss: 0.477317, acc.: 74.22%] [G loss: 4.296755]\n",
      "390 [D loss: 0.451916, acc.: 75.39%] [G loss: 4.457350]\n",
      "395 [D loss: 0.431399, acc.: 77.34%] [G loss: 3.918573]\n",
      "400 [D loss: 0.427202, acc.: 75.39%] [G loss: 4.303219]\n",
      "405 [D loss: 0.428377, acc.: 76.17%] [G loss: 4.147335]\n",
      "410 [D loss: 0.423270, acc.: 77.73%] [G loss: 4.460546]\n",
      "415 [D loss: 0.452049, acc.: 74.22%] [G loss: 4.425511]\n",
      "420 [D loss: 0.502659, acc.: 75.78%] [G loss: 4.360487]\n",
      "425 [D loss: 0.527631, acc.: 72.66%] [G loss: 4.349997]\n",
      "430 [D loss: 0.518449, acc.: 74.22%] [G loss: 4.645249]\n",
      "435 [D loss: 0.516917, acc.: 73.44%] [G loss: 4.437284]\n",
      "440 [D loss: 0.519043, acc.: 72.27%] [G loss: 4.835845]\n",
      "445 [D loss: 0.496898, acc.: 73.44%] [G loss: 4.087492]\n",
      "450 [D loss: 0.493941, acc.: 73.05%] [G loss: 4.239422]\n",
      "455 [D loss: 0.486116, acc.: 73.83%] [G loss: 4.143789]\n",
      "460 [D loss: 0.520932, acc.: 69.53%] [G loss: 4.042565]\n",
      "465 [D loss: 0.508326, acc.: 70.31%] [G loss: 3.790582]\n",
      "470 [D loss: 0.510336, acc.: 69.53%] [G loss: 3.903041]\n",
      "475 [D loss: 0.526149, acc.: 67.97%] [G loss: 3.919347]\n",
      "480 [D loss: 0.498199, acc.: 71.09%] [G loss: 4.132009]\n",
      "485 [D loss: 0.511883, acc.: 69.14%] [G loss: 4.384625]\n",
      "490 [D loss: 0.521516, acc.: 67.58%] [G loss: 3.793654]\n",
      "495 [D loss: 0.520674, acc.: 68.36%] [G loss: 3.541232]\n",
      "500 [D loss: 0.568543, acc.: 63.67%] [G loss: 3.323298]\n",
      "505 [D loss: 0.532109, acc.: 67.97%] [G loss: 3.321005]\n",
      "510 [D loss: 0.553827, acc.: 66.41%] [G loss: 2.715435]\n",
      "515 [D loss: 0.574798, acc.: 64.84%] [G loss: 3.094057]\n",
      "520 [D loss: 0.592254, acc.: 64.06%] [G loss: 2.841628]\n",
      "525 [D loss: 0.573431, acc.: 66.80%] [G loss: 2.851674]\n",
      "530 [D loss: 0.597327, acc.: 66.02%] [G loss: 3.327698]\n",
      "535 [D loss: 0.607760, acc.: 66.80%] [G loss: 3.006035]\n",
      "540 [D loss: 0.651418, acc.: 62.89%] [G loss: 2.621344]\n",
      "545 [D loss: 0.646995, acc.: 63.28%] [G loss: 2.466470]\n",
      "550 [D loss: 0.668983, acc.: 61.33%] [G loss: 2.878720]\n",
      "555 [D loss: 0.636151, acc.: 64.06%] [G loss: 2.973991]\n",
      "560 [D loss: 0.616338, acc.: 65.62%] [G loss: 2.764991]\n",
      "565 [D loss: 0.633725, acc.: 63.28%] [G loss: 2.537081]\n",
      "570 [D loss: 0.613684, acc.: 64.45%] [G loss: 2.506646]\n",
      "575 [D loss: 0.622161, acc.: 63.28%] [G loss: 2.951077]\n",
      "580 [D loss: 0.605733, acc.: 64.06%] [G loss: 2.591238]\n",
      "585 [D loss: 0.616345, acc.: 62.50%] [G loss: 2.561705]\n",
      "590 [D loss: 0.609374, acc.: 62.89%] [G loss: 2.665221]\n",
      "595 [D loss: 0.603034, acc.: 62.50%] [G loss: 2.600183]\n",
      "600 [D loss: 0.607116, acc.: 61.72%] [G loss: 2.583337]\n",
      "605 [D loss: 0.577589, acc.: 64.84%] [G loss: 2.284059]\n",
      "610 [D loss: 0.560880, acc.: 66.41%] [G loss: 2.828004]\n",
      "615 [D loss: 0.591986, acc.: 62.50%] [G loss: 2.442145]\n",
      "620 [D loss: 0.568273, acc.: 63.28%] [G loss: 2.521678]\n",
      "625 [D loss: 0.547311, acc.: 64.45%] [G loss: 2.716812]\n",
      "630 [D loss: 0.562868, acc.: 64.84%] [G loss: 2.599005]\n",
      "635 [D loss: 0.570038, acc.: 62.89%] [G loss: 2.550105]\n",
      "640 [D loss: 0.583100, acc.: 60.55%] [G loss: 2.295927]\n",
      "645 [D loss: 0.521359, acc.: 65.23%] [G loss: 2.211445]\n",
      "650 [D loss: 0.568864, acc.: 61.33%] [G loss: 2.159984]\n",
      "655 [D loss: 0.476049, acc.: 63.67%] [G loss: 2.059969]\n",
      "660 [D loss: 0.538830, acc.: 90.62%] [G loss: 5.988860]\n",
      "665 [D loss: 1.015607, acc.: 59.38%] [G loss: 2.389444]\n",
      "670 [D loss: 0.643708, acc.: 50.00%] [G loss: 0.974919]\n",
      "675 [D loss: 0.640332, acc.: 48.44%] [G loss: 1.011280]\n",
      "680 [D loss: 0.632482, acc.: 48.05%] [G loss: 1.035654]\n",
      "685 [D loss: 0.641472, acc.: 48.83%] [G loss: 1.146334]\n",
      "690 [D loss: 0.619538, acc.: 48.83%] [G loss: 1.192228]\n",
      "695 [D loss: 0.613827, acc.: 50.39%] [G loss: 1.325487]\n",
      "700 [D loss: 0.615349, acc.: 50.00%] [G loss: 1.379946]\n",
      "705 [D loss: 0.616629, acc.: 49.61%] [G loss: 1.516624]\n",
      "710 [D loss: 0.606246, acc.: 50.39%] [G loss: 1.507948]\n",
      "715 [D loss: 0.592286, acc.: 48.44%] [G loss: 1.646784]\n",
      "720 [D loss: 0.620005, acc.: 48.44%] [G loss: 1.654665]\n",
      "725 [D loss: 0.607749, acc.: 49.61%] [G loss: 1.771780]\n",
      "730 [D loss: 0.604718, acc.: 51.17%] [G loss: 1.770640]\n",
      "735 [D loss: 0.612226, acc.: 48.83%] [G loss: 1.690118]\n",
      "740 [D loss: 0.581688, acc.: 48.83%] [G loss: 1.713871]\n",
      "745 [D loss: 0.595028, acc.: 51.17%] [G loss: 2.078437]\n",
      "750 [D loss: 0.560981, acc.: 53.12%] [G loss: 2.087590]\n",
      "755 [D loss: 0.540052, acc.: 53.52%] [G loss: 2.422250]\n",
      "760 [D loss: 0.490172, acc.: 58.20%] [G loss: 2.554574]\n",
      "765 [D loss: 0.496412, acc.: 57.03%] [G loss: 2.610487]\n",
      "770 [D loss: 0.514576, acc.: 64.45%] [G loss: 2.675317]\n",
      "775 [D loss: 0.502349, acc.: 70.31%] [G loss: 2.922076]\n",
      "780 [D loss: 0.503285, acc.: 80.08%] [G loss: 2.840056]\n",
      "785 [D loss: 0.501991, acc.: 84.77%] [G loss: 2.543871]\n",
      "790 [D loss: 0.529773, acc.: 82.81%] [G loss: 2.653661]\n",
      "795 [D loss: 0.542673, acc.: 87.11%] [G loss: 2.795622]\n",
      "800 [D loss: 0.531488, acc.: 88.67%] [G loss: 2.310281]\n",
      "805 [D loss: 0.569324, acc.: 83.98%] [G loss: 2.553522]\n",
      "810 [D loss: 0.566020, acc.: 80.47%] [G loss: 2.427863]\n",
      "815 [D loss: 0.579217, acc.: 77.73%] [G loss: 2.487624]\n",
      "820 [D loss: 0.586567, acc.: 80.08%] [G loss: 2.524723]\n",
      "825 [D loss: 0.577235, acc.: 83.98%] [G loss: 2.296878]\n",
      "830 [D loss: 0.597387, acc.: 82.03%] [G loss: 2.413176]\n",
      "835 [D loss: 0.559289, acc.: 89.84%] [G loss: 2.631759]\n",
      "840 [D loss: 0.550565, acc.: 90.23%] [G loss: 3.040408]\n",
      "845 [D loss: 0.496299, acc.: 92.97%] [G loss: 2.867411]\n",
      "850 [D loss: 0.703371, acc.: 69.14%] [G loss: 3.010014]\n",
      "855 [D loss: 0.619210, acc.: 59.38%] [G loss: 2.213982]\n",
      "860 [D loss: 0.588573, acc.: 62.11%] [G loss: 2.102785]\n",
      "865 [D loss: 0.621962, acc.: 60.16%] [G loss: 2.007624]\n",
      "870 [D loss: 0.672329, acc.: 55.08%] [G loss: 2.065141]\n",
      "875 [D loss: 0.601270, acc.: 61.72%] [G loss: 2.162018]\n",
      "880 [D loss: 0.589515, acc.: 63.28%] [G loss: 2.364367]\n",
      "885 [D loss: 0.602256, acc.: 66.80%] [G loss: 2.460505]\n",
      "890 [D loss: 0.542406, acc.: 82.81%] [G loss: 2.744128]\n",
      "895 [D loss: 0.558054, acc.: 82.81%] [G loss: 3.023209]\n",
      "900 [D loss: 0.574682, acc.: 82.81%] [G loss: 2.395934]\n",
      "905 [D loss: 0.578773, acc.: 86.33%] [G loss: 2.246517]\n",
      "910 [D loss: 0.589979, acc.: 79.30%] [G loss: 2.361185]\n",
      "915 [D loss: 0.547847, acc.: 89.06%] [G loss: 2.310719]\n",
      "920 [D loss: 0.555824, acc.: 82.42%] [G loss: 2.525585]\n",
      "925 [D loss: 0.542369, acc.: 80.08%] [G loss: 2.609257]\n",
      "930 [D loss: 0.536792, acc.: 81.25%] [G loss: 2.960821]\n",
      "935 [D loss: 0.460202, acc.: 86.72%] [G loss: 2.540871]\n",
      "940 [D loss: 0.371390, acc.: 92.19%] [G loss: 5.259302]\n",
      "945 [D loss: 0.468851, acc.: 81.64%] [G loss: 3.866750]\n",
      "950 [D loss: 0.531465, acc.: 67.97%] [G loss: 2.829651]\n",
      "955 [D loss: 0.726144, acc.: 58.20%] [G loss: 1.945748]\n",
      "960 [D loss: 0.615792, acc.: 61.33%] [G loss: 1.962056]\n",
      "965 [D loss: 0.249697, acc.: 93.75%] [G loss: 3.331332]\n",
      "970 [D loss: 0.593598, acc.: 57.03%] [G loss: 1.674021]\n",
      "975 [D loss: 0.552034, acc.: 59.38%] [G loss: 2.023047]\n",
      "980 [D loss: 0.530864, acc.: 66.80%] [G loss: 1.793262]\n",
      "985 [D loss: 0.533374, acc.: 74.61%] [G loss: 1.700475]\n",
      "990 [D loss: 0.531370, acc.: 85.94%] [G loss: 1.694449]\n",
      "995 [D loss: 0.496981, acc.: 89.84%] [G loss: 1.680954]\n",
      "1000 [D loss: 0.458212, acc.: 93.75%] [G loss: 1.880628]\n",
      "1005 [D loss: 0.425031, acc.: 95.31%] [G loss: 1.801660]\n",
      "1010 [D loss: 0.447581, acc.: 92.97%] [G loss: 1.894438]\n",
      "1015 [D loss: 0.455226, acc.: 91.41%] [G loss: 2.025727]\n",
      "1020 [D loss: 0.482340, acc.: 89.84%] [G loss: 1.911631]\n",
      "1025 [D loss: 0.461962, acc.: 92.19%] [G loss: 2.141560]\n",
      "1030 [D loss: 0.401656, acc.: 94.14%] [G loss: 1.901325]\n",
      "1035 [D loss: 0.348612, acc.: 84.77%] [G loss: 2.948460]\n",
      "1040 [D loss: 1.088765, acc.: 80.08%] [G loss: 1.865417]\n",
      "1045 [D loss: 0.816442, acc.: 59.77%] [G loss: 0.572937]\n",
      "1050 [D loss: 0.639156, acc.: 83.98%] [G loss: 1.401905]\n",
      "1055 [D loss: 0.501439, acc.: 86.33%] [G loss: 1.338359]\n",
      "1060 [D loss: 0.583520, acc.: 83.59%] [G loss: 1.161851]\n",
      "1065 [D loss: 0.504782, acc.: 85.94%] [G loss: 1.295054]\n",
      "1070 [D loss: 0.492474, acc.: 85.16%] [G loss: 1.440885]\n",
      "1075 [D loss: 0.471655, acc.: 86.33%] [G loss: 1.601228]\n",
      "1080 [D loss: 0.478101, acc.: 87.50%] [G loss: 1.796696]\n",
      "1085 [D loss: 0.406487, acc.: 90.62%] [G loss: 1.781843]\n",
      "1090 [D loss: 0.464896, acc.: 83.98%] [G loss: 2.078628]\n",
      "1095 [D loss: 0.475223, acc.: 84.38%] [G loss: 1.947096]\n",
      "1100 [D loss: 0.470564, acc.: 81.25%] [G loss: 1.739618]\n",
      "1105 [D loss: 0.460494, acc.: 84.77%] [G loss: 2.159329]\n",
      "1110 [D loss: 0.488133, acc.: 82.42%] [G loss: 2.000899]\n",
      "1115 [D loss: 0.536120, acc.: 79.30%] [G loss: 1.832751]\n",
      "1120 [D loss: 0.464986, acc.: 86.72%] [G loss: 1.759754]\n",
      "1125 [D loss: 0.464907, acc.: 87.11%] [G loss: 1.578534]\n",
      "1130 [D loss: 0.435359, acc.: 87.89%] [G loss: 1.740387]\n",
      "1135 [D loss: 0.392032, acc.: 89.84%] [G loss: 2.317481]\n",
      "1140 [D loss: 0.554832, acc.: 77.34%] [G loss: 1.805077]\n",
      "1145 [D loss: 0.715775, acc.: 66.41%] [G loss: 2.736869]\n",
      "1150 [D loss: 0.460639, acc.: 81.25%] [G loss: 2.909294]\n",
      "1155 [D loss: 0.281085, acc.: 95.70%] [G loss: 3.939680]\n",
      "1160 [D loss: 0.261376, acc.: 93.75%] [G loss: 5.358209]\n",
      "1165 [D loss: 1.443688, acc.: 27.34%] [G loss: 1.938117]\n",
      "1170 [D loss: 0.451595, acc.: 86.72%] [G loss: 2.700414]\n",
      "1175 [D loss: 0.317349, acc.: 94.14%] [G loss: 3.309671]\n",
      "1180 [D loss: 0.266854, acc.: 98.05%] [G loss: 4.376705]\n",
      "1185 [D loss: 0.200819, acc.: 100.00%] [G loss: 4.101984]\n",
      "1190 [D loss: 0.199035, acc.: 100.00%] [G loss: 4.471088]\n",
      "1195 [D loss: 0.166436, acc.: 100.00%] [G loss: 4.545681]\n",
      "1200 [D loss: 0.170707, acc.: 99.61%] [G loss: 4.708969]\n",
      "1205 [D loss: 0.199824, acc.: 98.44%] [G loss: 5.266558]\n",
      "1210 [D loss: 0.159587, acc.: 98.83%] [G loss: 4.976292]\n",
      "1215 [D loss: 0.350034, acc.: 86.72%] [G loss: 5.255461]\n",
      "1220 [D loss: 0.236101, acc.: 92.58%] [G loss: 5.836086]\n",
      "1225 [D loss: 0.232167, acc.: 93.75%] [G loss: 6.317485]\n",
      "1230 [D loss: 0.552190, acc.: 73.05%] [G loss: 5.775417]\n",
      "1235 [D loss: 0.501844, acc.: 79.69%] [G loss: 6.962927]\n",
      "1240 [D loss: 0.833817, acc.: 63.28%] [G loss: 5.863030]\n",
      "1245 [D loss: 0.484723, acc.: 73.83%] [G loss: 5.811630]\n",
      "1250 [D loss: 0.375698, acc.: 76.56%] [G loss: 6.889037]\n",
      "1255 [D loss: 0.232670, acc.: 97.27%] [G loss: 6.670448]\n",
      "1260 [D loss: 0.163736, acc.: 94.53%] [G loss: 7.415011]\n",
      "1265 [D loss: 1.090306, acc.: 71.09%] [G loss: 5.379694]\n",
      "1270 [D loss: 0.334014, acc.: 83.59%] [G loss: 3.529114]\n",
      "1275 [D loss: 0.322808, acc.: 89.06%] [G loss: 4.376874]\n",
      "1280 [D loss: 0.250345, acc.: 91.41%] [G loss: 4.489556]\n",
      "1285 [D loss: 0.230579, acc.: 93.36%] [G loss: 5.312175]\n",
      "1290 [D loss: 0.229005, acc.: 91.41%] [G loss: 4.782809]\n",
      "1295 [D loss: 0.221905, acc.: 90.23%] [G loss: 4.396260]\n",
      "1300 [D loss: 0.148168, acc.: 96.48%] [G loss: 4.805922]\n",
      "1305 [D loss: 0.181968, acc.: 93.75%] [G loss: 4.756126]\n",
      "1310 [D loss: 0.152571, acc.: 94.92%] [G loss: 5.543451]\n",
      "1315 [D loss: 0.086748, acc.: 98.83%] [G loss: 5.553484]\n",
      "1320 [D loss: 0.237188, acc.: 88.28%] [G loss: 5.522490]\n",
      "1325 [D loss: 0.164581, acc.: 96.09%] [G loss: 6.035172]\n",
      "1330 [D loss: 1.538102, acc.: 69.14%] [G loss: 4.157554]\n",
      "1335 [D loss: 0.520598, acc.: 75.00%] [G loss: 5.446231]\n",
      "1340 [D loss: 0.494020, acc.: 73.44%] [G loss: 4.066215]\n",
      "1345 [D loss: 0.302051, acc.: 87.50%] [G loss: 4.359289]\n",
      "1350 [D loss: 0.231473, acc.: 91.41%] [G loss: 4.482130]\n",
      "1355 [D loss: 0.232612, acc.: 89.84%] [G loss: 4.937212]\n",
      "1360 [D loss: 0.179733, acc.: 93.75%] [G loss: 5.639375]\n",
      "1365 [D loss: 0.295233, acc.: 88.67%] [G loss: 5.453842]\n",
      "1370 [D loss: 0.261650, acc.: 90.23%] [G loss: 5.571648]\n",
      "1375 [D loss: 0.087868, acc.: 98.44%] [G loss: 8.139441]\n",
      "1380 [D loss: 0.401489, acc.: 80.08%] [G loss: 7.621705]\n",
      "1385 [D loss: 0.453057, acc.: 79.30%] [G loss: 5.521682]\n",
      "1390 [D loss: 0.233037, acc.: 94.14%] [G loss: 4.812407]\n",
      "1395 [D loss: 0.179522, acc.: 94.92%] [G loss: 6.324252]\n",
      "1400 [D loss: 0.127026, acc.: 93.75%] [G loss: 6.337782]\n",
      "1405 [D loss: 0.102519, acc.: 97.27%] [G loss: 6.485528]\n",
      "1410 [D loss: 0.155835, acc.: 94.92%] [G loss: 6.672110]\n",
      "1415 [D loss: 0.174933, acc.: 96.09%] [G loss: 7.534563]\n",
      "1420 [D loss: 0.218721, acc.: 89.84%] [G loss: 7.820359]\n",
      "1425 [D loss: 0.079436, acc.: 98.83%] [G loss: 7.862965]\n",
      "1430 [D loss: 0.022196, acc.: 100.00%] [G loss: 8.037878]\n",
      "1435 [D loss: 0.148178, acc.: 97.27%] [G loss: 7.635550]\n",
      "1440 [D loss: 0.160932, acc.: 98.44%] [G loss: 9.613030]\n",
      "1445 [D loss: 1.610123, acc.: 65.23%] [G loss: 7.480416]\n",
      "1450 [D loss: 0.540290, acc.: 78.52%] [G loss: 5.225475]\n",
      "1455 [D loss: 0.060137, acc.: 98.05%] [G loss: 5.818177]\n",
      "1460 [D loss: 0.200689, acc.: 92.19%] [G loss: 5.286504]\n",
      "1465 [D loss: 0.247782, acc.: 94.92%] [G loss: 4.879372]\n",
      "1470 [D loss: 0.197268, acc.: 91.80%] [G loss: 3.967971]\n",
      "1475 [D loss: 0.154802, acc.: 97.27%] [G loss: 4.869511]\n",
      "1480 [D loss: 0.157960, acc.: 96.48%] [G loss: 4.926631]\n",
      "1485 [D loss: 0.343868, acc.: 93.75%] [G loss: 5.605424]\n",
      "1490 [D loss: 0.149809, acc.: 95.70%] [G loss: 5.403671]\n",
      "1495 [D loss: 0.122379, acc.: 97.66%] [G loss: 5.422085]\n",
      "1500 [D loss: 0.134784, acc.: 95.70%] [G loss: 5.745783]\n",
      "1505 [D loss: 0.231656, acc.: 93.36%] [G loss: 6.354211]\n",
      "1510 [D loss: 0.418417, acc.: 87.50%] [G loss: 5.212808]\n",
      "1515 [D loss: 0.696465, acc.: 74.22%] [G loss: 4.032565]\n",
      "1520 [D loss: 0.742764, acc.: 71.88%] [G loss: 3.789167]\n",
      "1525 [D loss: 0.349143, acc.: 89.45%] [G loss: 3.912461]\n",
      "1530 [D loss: 0.200228, acc.: 92.58%] [G loss: 4.202507]\n",
      "1535 [D loss: 0.268057, acc.: 94.14%] [G loss: 4.671757]\n",
      "1540 [D loss: 0.119736, acc.: 97.27%] [G loss: 5.456660]\n",
      "1545 [D loss: 0.367535, acc.: 80.86%] [G loss: 4.013174]\n",
      "1550 [D loss: 0.147537, acc.: 96.88%] [G loss: 4.754040]\n",
      "1555 [D loss: 0.150540, acc.: 93.36%] [G loss: 4.794473]\n",
      "1560 [D loss: 0.602490, acc.: 74.61%] [G loss: 3.923132]\n",
      "1565 [D loss: 2.780451, acc.: 50.00%] [G loss: 6.152624]\n",
      "1570 [D loss: 1.120317, acc.: 49.22%] [G loss: 1.226248]\n",
      "1575 [D loss: 0.849481, acc.: 43.36%] [G loss: 0.598680]\n",
      "1580 [D loss: 0.685499, acc.: 57.03%] [G loss: 0.760752]\n",
      "1585 [D loss: 0.665894, acc.: 60.16%] [G loss: 0.854330]\n",
      "1590 [D loss: 0.644328, acc.: 60.16%] [G loss: 0.864662]\n",
      "1595 [D loss: 0.658662, acc.: 64.06%] [G loss: 0.966729]\n",
      "1600 [D loss: 0.627098, acc.: 63.67%] [G loss: 0.848537]\n",
      "1605 [D loss: 0.618711, acc.: 64.84%] [G loss: 0.913594]\n",
      "1610 [D loss: 0.633638, acc.: 60.94%] [G loss: 0.928622]\n",
      "1615 [D loss: 0.636041, acc.: 62.50%] [G loss: 0.904389]\n",
      "1620 [D loss: 0.597774, acc.: 70.31%] [G loss: 1.107582]\n",
      "1625 [D loss: 0.597385, acc.: 66.80%] [G loss: 1.160033]\n",
      "1630 [D loss: 0.592446, acc.: 64.06%] [G loss: 1.133970]\n",
      "1635 [D loss: 0.603381, acc.: 67.19%] [G loss: 1.096656]\n",
      "1640 [D loss: 0.559830, acc.: 70.70%] [G loss: 1.063957]\n",
      "1645 [D loss: 0.592857, acc.: 67.97%] [G loss: 1.011883]\n",
      "1650 [D loss: 0.581228, acc.: 67.19%] [G loss: 0.992024]\n",
      "1655 [D loss: 0.577321, acc.: 71.48%] [G loss: 1.043667]\n",
      "1660 [D loss: 0.595098, acc.: 66.80%] [G loss: 1.129264]\n",
      "1665 [D loss: 0.599261, acc.: 66.80%] [G loss: 1.081271]\n",
      "1670 [D loss: 0.572339, acc.: 66.80%] [G loss: 1.028448]\n",
      "1675 [D loss: 0.561108, acc.: 72.27%] [G loss: 0.988349]\n",
      "1680 [D loss: 0.575470, acc.: 69.92%] [G loss: 1.025286]\n",
      "1685 [D loss: 0.560831, acc.: 72.27%] [G loss: 1.111040]\n",
      "1690 [D loss: 0.559440, acc.: 75.00%] [G loss: 1.154875]\n",
      "1695 [D loss: 0.569587, acc.: 73.44%] [G loss: 1.080346]\n",
      "1700 [D loss: 0.578716, acc.: 69.92%] [G loss: 1.078304]\n",
      "1705 [D loss: 0.563110, acc.: 67.58%] [G loss: 1.173420]\n",
      "1710 [D loss: 0.537606, acc.: 74.22%] [G loss: 1.155765]\n",
      "1715 [D loss: 0.518339, acc.: 75.00%] [G loss: 1.199293]\n",
      "1720 [D loss: 0.550705, acc.: 73.44%] [G loss: 1.191992]\n",
      "1725 [D loss: 0.579262, acc.: 70.70%] [G loss: 1.204242]\n",
      "1730 [D loss: 0.544410, acc.: 71.48%] [G loss: 1.373951]\n",
      "1735 [D loss: 0.533381, acc.: 71.09%] [G loss: 1.325472]\n",
      "1740 [D loss: 0.537724, acc.: 73.05%] [G loss: 1.268814]\n",
      "1745 [D loss: 0.519941, acc.: 77.34%] [G loss: 1.344252]\n",
      "1750 [D loss: 0.559924, acc.: 69.92%] [G loss: 1.242642]\n",
      "1755 [D loss: 0.529889, acc.: 75.39%] [G loss: 1.368320]\n",
      "1760 [D loss: 0.528091, acc.: 71.88%] [G loss: 1.336955]\n",
      "1765 [D loss: 0.525546, acc.: 74.61%] [G loss: 1.279390]\n",
      "1770 [D loss: 0.523411, acc.: 74.61%] [G loss: 1.372787]\n",
      "1775 [D loss: 0.556683, acc.: 70.31%] [G loss: 1.423993]\n",
      "1780 [D loss: 0.542708, acc.: 71.88%] [G loss: 1.446765]\n",
      "1785 [D loss: 0.556763, acc.: 71.88%] [G loss: 1.387708]\n",
      "1790 [D loss: 0.555439, acc.: 69.53%] [G loss: 1.134001]\n",
      "1795 [D loss: 0.572461, acc.: 66.02%] [G loss: 1.301425]\n",
      "1800 [D loss: 0.567665, acc.: 66.80%] [G loss: 1.274459]\n",
      "1805 [D loss: 0.556900, acc.: 67.19%] [G loss: 1.355199]\n",
      "1810 [D loss: 0.584597, acc.: 62.89%] [G loss: 1.250438]\n",
      "1815 [D loss: 0.574668, acc.: 66.02%] [G loss: 1.235499]\n",
      "1820 [D loss: 2.264077, acc.: 50.00%] [G loss: 4.474476]\n",
      "1825 [D loss: 1.021891, acc.: 33.98%] [G loss: 0.693618]\n",
      "1830 [D loss: 0.751015, acc.: 56.25%] [G loss: 0.547358]\n",
      "1835 [D loss: 0.701025, acc.: 57.03%] [G loss: 0.696303]\n",
      "1840 [D loss: 0.678815, acc.: 55.86%] [G loss: 0.856610]\n",
      "1845 [D loss: 0.624537, acc.: 59.77%] [G loss: 1.069932]\n",
      "1850 [D loss: 0.614983, acc.: 62.89%] [G loss: 1.040504]\n",
      "1855 [D loss: 0.628910, acc.: 64.84%] [G loss: 1.132064]\n",
      "1860 [D loss: 0.577590, acc.: 69.92%] [G loss: 1.172294]\n",
      "1865 [D loss: 0.594181, acc.: 69.14%] [G loss: 1.303530]\n",
      "1870 [D loss: 0.542718, acc.: 75.39%] [G loss: 1.326700]\n",
      "1875 [D loss: 0.543759, acc.: 71.88%] [G loss: 1.112482]\n",
      "1880 [D loss: 0.563098, acc.: 69.92%] [G loss: 1.224596]\n",
      "1885 [D loss: 0.522644, acc.: 72.27%] [G loss: 1.154242]\n",
      "1890 [D loss: 0.532004, acc.: 73.05%] [G loss: 1.141544]\n",
      "1895 [D loss: 0.559957, acc.: 71.09%] [G loss: 1.090029]\n",
      "1900 [D loss: 0.523367, acc.: 75.78%] [G loss: 1.088945]\n",
      "1905 [D loss: 0.566962, acc.: 71.48%] [G loss: 1.012323]\n",
      "1910 [D loss: 0.557038, acc.: 69.92%] [G loss: 1.085618]\n",
      "1915 [D loss: 0.512880, acc.: 72.66%] [G loss: 1.363361]\n",
      "1920 [D loss: 0.541487, acc.: 70.31%] [G loss: 1.235514]\n",
      "1925 [D loss: 0.551498, acc.: 75.00%] [G loss: 1.339465]\n",
      "1930 [D loss: 0.527474, acc.: 71.48%] [G loss: 1.411249]\n",
      "1935 [D loss: 0.671371, acc.: 57.42%] [G loss: 1.438305]\n",
      "1940 [D loss: 0.565133, acc.: 70.31%] [G loss: 1.728304]\n",
      "1945 [D loss: 0.677867, acc.: 68.75%] [G loss: 1.271383]\n",
      "1950 [D loss: 0.763328, acc.: 53.52%] [G loss: 1.082937]\n",
      "1955 [D loss: 0.570975, acc.: 67.58%] [G loss: 1.413032]\n",
      "1960 [D loss: 0.545680, acc.: 64.45%] [G loss: 2.177686]\n",
      "1965 [D loss: 0.557184, acc.: 65.23%] [G loss: 2.100742]\n",
      "1970 [D loss: 0.506823, acc.: 76.56%] [G loss: 2.124715]\n",
      "1975 [D loss: 0.462249, acc.: 74.22%] [G loss: 2.450458]\n",
      "1980 [D loss: 0.745022, acc.: 44.92%] [G loss: 1.861818]\n",
      "1985 [D loss: 0.593967, acc.: 61.33%] [G loss: 1.678436]\n",
      "1990 [D loss: 0.585828, acc.: 63.28%] [G loss: 1.827600]\n",
      "1995 [D loss: 0.613476, acc.: 61.33%] [G loss: 1.739295]\n",
      "2000 [D loss: 0.602340, acc.: 63.67%] [G loss: 1.773097]\n",
      "2005 [D loss: 0.646534, acc.: 55.86%] [G loss: 1.811098]\n",
      "2010 [D loss: 0.570213, acc.: 63.67%] [G loss: 2.002426]\n",
      "2015 [D loss: 0.515883, acc.: 71.09%] [G loss: 2.172433]\n",
      "2020 [D loss: 0.484597, acc.: 74.61%] [G loss: 2.234424]\n",
      "2025 [D loss: 0.546818, acc.: 70.70%] [G loss: 2.245089]\n",
      "2030 [D loss: 0.536118, acc.: 66.41%] [G loss: 2.120514]\n",
      "2035 [D loss: 0.543812, acc.: 65.62%] [G loss: 2.196569]\n",
      "2040 [D loss: 0.491378, acc.: 76.56%] [G loss: 2.541915]\n",
      "2045 [D loss: 0.518185, acc.: 73.83%] [G loss: 2.458122]\n",
      "2050 [D loss: 0.503307, acc.: 71.48%] [G loss: 2.004627]\n",
      "2055 [D loss: 0.630594, acc.: 64.45%] [G loss: 1.883135]\n",
      "2060 [D loss: 0.564958, acc.: 67.97%] [G loss: 2.114576]\n",
      "2065 [D loss: 0.623697, acc.: 58.20%] [G loss: 2.052521]\n",
      "2070 [D loss: 0.492601, acc.: 75.00%] [G loss: 2.472221]\n",
      "2075 [D loss: 0.335842, acc.: 92.58%] [G loss: 3.583555]\n",
      "2080 [D loss: 0.666838, acc.: 54.69%] [G loss: 1.867303]\n",
      "2085 [D loss: 0.620421, acc.: 59.38%] [G loss: 2.069211]\n",
      "2090 [D loss: 0.572314, acc.: 63.67%] [G loss: 1.864839]\n",
      "2095 [D loss: 0.486590, acc.: 73.83%] [G loss: 1.976788]\n",
      "2100 [D loss: 0.558785, acc.: 68.36%] [G loss: 1.859763]\n",
      "2105 [D loss: 0.490228, acc.: 73.44%] [G loss: 1.882697]\n",
      "2110 [D loss: 0.411864, acc.: 82.42%] [G loss: 2.145502]\n",
      "2115 [D loss: 0.326279, acc.: 93.36%] [G loss: 2.192953]\n",
      "2120 [D loss: 0.198372, acc.: 98.44%] [G loss: 2.760859]\n",
      "2125 [D loss: 0.141869, acc.: 98.05%] [G loss: 3.116953]\n",
      "2130 [D loss: 0.236811, acc.: 94.53%] [G loss: 3.221605]\n",
      "2135 [D loss: 0.299485, acc.: 87.11%] [G loss: 2.330559]\n",
      "2140 [D loss: 0.282732, acc.: 89.06%] [G loss: 2.757554]\n",
      "2145 [D loss: 0.234464, acc.: 92.58%] [G loss: 3.841049]\n",
      "2150 [D loss: 0.182231, acc.: 96.48%] [G loss: 3.295235]\n",
      "2155 [D loss: 0.126557, acc.: 96.48%] [G loss: 3.556891]\n",
      "2160 [D loss: 0.114978, acc.: 96.48%] [G loss: 3.819680]\n",
      "2165 [D loss: 0.068758, acc.: 98.44%] [G loss: 3.766886]\n",
      "2170 [D loss: 0.081018, acc.: 97.66%] [G loss: 4.065768]\n",
      "2175 [D loss: 0.091782, acc.: 98.05%] [G loss: 4.480745]\n",
      "2180 [D loss: 0.088971, acc.: 97.27%] [G loss: 4.124510]\n",
      "2185 [D loss: 0.110678, acc.: 98.83%] [G loss: 4.319973]\n",
      "2190 [D loss: 0.086465, acc.: 98.05%] [G loss: 4.278530]\n",
      "2195 [D loss: 0.099897, acc.: 97.27%] [G loss: 4.356619]\n",
      "2200 [D loss: 0.064760, acc.: 98.05%] [G loss: 4.254681]\n",
      "2205 [D loss: 0.183721, acc.: 93.36%] [G loss: 3.482213]\n",
      "2210 [D loss: 0.123747, acc.: 95.70%] [G loss: 3.071255]\n",
      "2215 [D loss: 0.099402, acc.: 97.27%] [G loss: 4.037956]\n",
      "2220 [D loss: 0.110382, acc.: 97.27%] [G loss: 4.289485]\n",
      "2225 [D loss: 0.110740, acc.: 96.88%] [G loss: 3.679110]\n",
      "2230 [D loss: 0.125912, acc.: 94.14%] [G loss: 4.072550]\n",
      "2235 [D loss: 0.177518, acc.: 95.31%] [G loss: 3.752960]\n",
      "2240 [D loss: 0.047049, acc.: 99.61%] [G loss: 3.563284]\n",
      "2245 [D loss: 0.118904, acc.: 97.66%] [G loss: 3.955922]\n",
      "2250 [D loss: 0.059183, acc.: 99.22%] [G loss: 3.799460]\n",
      "2255 [D loss: 0.038065, acc.: 99.61%] [G loss: 4.155581]\n",
      "2260 [D loss: 0.065601, acc.: 98.83%] [G loss: 3.783982]\n",
      "2265 [D loss: 0.103927, acc.: 98.05%] [G loss: 3.740860]\n",
      "2270 [D loss: 0.086663, acc.: 96.48%] [G loss: 4.832342]\n",
      "2275 [D loss: 0.066472, acc.: 98.44%] [G loss: 4.491560]\n",
      "2280 [D loss: 0.085223, acc.: 97.66%] [G loss: 4.468088]\n",
      "2285 [D loss: 0.088072, acc.: 98.05%] [G loss: 4.233562]\n",
      "2290 [D loss: 0.056236, acc.: 98.83%] [G loss: 4.651567]\n",
      "2295 [D loss: 0.046803, acc.: 98.83%] [G loss: 4.502434]\n",
      "2300 [D loss: 0.393502, acc.: 88.67%] [G loss: 5.138289]\n",
      "2305 [D loss: 0.349317, acc.: 87.89%] [G loss: 3.598989]\n",
      "2310 [D loss: 0.205128, acc.: 95.31%] [G loss: 4.255137]\n",
      "2315 [D loss: 0.124375, acc.: 98.05%] [G loss: 4.457444]\n",
      "2320 [D loss: 0.097048, acc.: 98.05%] [G loss: 3.557088]\n",
      "2325 [D loss: 0.115101, acc.: 97.27%] [G loss: 3.700586]\n",
      "2330 [D loss: 0.095573, acc.: 97.66%] [G loss: 4.138868]\n",
      "2335 [D loss: 0.068663, acc.: 98.83%] [G loss: 3.797495]\n",
      "2340 [D loss: 0.060530, acc.: 98.83%] [G loss: 3.907015]\n",
      "2345 [D loss: 0.039266, acc.: 100.00%] [G loss: 4.118700]\n",
      "2350 [D loss: 0.098577, acc.: 98.05%] [G loss: 4.266771]\n",
      "2355 [D loss: 0.038304, acc.: 98.83%] [G loss: 4.820395]\n",
      "2360 [D loss: 0.081098, acc.: 98.83%] [G loss: 4.946307]\n",
      "2365 [D loss: 0.045850, acc.: 99.61%] [G loss: 4.812105]\n",
      "2370 [D loss: 1.972091, acc.: 52.73%] [G loss: 3.991333]\n",
      "2375 [D loss: 0.986845, acc.: 41.02%] [G loss: 2.040381]\n",
      "2380 [D loss: 0.824481, acc.: 52.34%] [G loss: 1.670600]\n",
      "2385 [D loss: 0.804317, acc.: 49.61%] [G loss: 1.377797]\n",
      "2390 [D loss: 0.745251, acc.: 48.44%] [G loss: 1.928248]\n",
      "2395 [D loss: 0.709464, acc.: 52.73%] [G loss: 1.685664]\n",
      "2400 [D loss: 0.704745, acc.: 51.56%] [G loss: 1.747689]\n",
      "2405 [D loss: 0.688708, acc.: 57.03%] [G loss: 1.633998]\n",
      "2410 [D loss: 0.666599, acc.: 57.81%] [G loss: 1.663163]\n",
      "2415 [D loss: 0.673342, acc.: 57.03%] [G loss: 1.566019]\n",
      "2420 [D loss: 0.715619, acc.: 49.22%] [G loss: 1.465167]\n",
      "2425 [D loss: 0.652001, acc.: 57.81%] [G loss: 1.837467]\n",
      "2430 [D loss: 0.663174, acc.: 57.42%] [G loss: 1.795761]\n",
      "2435 [D loss: 0.613586, acc.: 59.38%] [G loss: 1.682378]\n",
      "2440 [D loss: 0.620413, acc.: 60.94%] [G loss: 1.594745]\n",
      "2445 [D loss: 0.639065, acc.: 64.84%] [G loss: 1.661849]\n",
      "2450 [D loss: 0.577848, acc.: 68.36%] [G loss: 1.682473]\n",
      "2455 [D loss: 0.563372, acc.: 72.27%] [G loss: 1.860321]\n",
      "2460 [D loss: 0.525318, acc.: 69.92%] [G loss: 1.766907]\n",
      "2465 [D loss: 0.482631, acc.: 76.17%] [G loss: 1.973450]\n",
      "2470 [D loss: 0.443421, acc.: 76.17%] [G loss: 2.332753]\n",
      "2475 [D loss: 0.443143, acc.: 80.47%] [G loss: 2.217241]\n",
      "2480 [D loss: 0.373557, acc.: 87.11%] [G loss: 2.434825]\n",
      "2485 [D loss: 0.355651, acc.: 89.06%] [G loss: 2.786888]\n",
      "2490 [D loss: 0.339700, acc.: 87.89%] [G loss: 3.096896]\n",
      "2495 [D loss: 0.224027, acc.: 95.70%] [G loss: 3.092718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:497: UserWarning: Unrecognized location \"middle right\". Falling back on \"best\"; valid locations are\n",
      "\tbest\n",
      "\tupper right\n",
      "\tupper left\n",
      "\tlower left\n",
      "\tlower right\n",
      "\tright\n",
      "\tcenter left\n",
      "\tcenter right\n",
      "\tlower center\n",
      "\tupper center\n",
      "\tcenter\n",
      "\n",
      "  % (loc, '\\n\\t'.join(self.codes)))\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(rows=100)    \n",
    "final_seqs= gan.train(epochs=2500, batch_size=128, sample_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hVHteipMY2aW",
    "outputId": "17b16dcf-ba9a-442f-b41b-d632ee948252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1302
    },
    "colab_type": "code",
    "id": "ofuMRtymnQaY",
    "outputId": "3ea751d0-cbfa-4d5c-9a26-ee35327bab5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 512)               1056768   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,188,353\n",
      "Trainable params: 1,188,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100, 3)            0         \n",
      "=================================================================\n",
      "Total params: 997,420\n",
      "Trainable params: 993,836\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 512)               1056768   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,188,353\n",
      "Trainable params: 1,188,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_t = GAN(rows=100)\n",
    "discriminator = gan_t.build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXPKc7-bY2kH"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model  \n",
    "plot_model(gan.combined, to_file= datapath + '/fcn-gen-arch.png')\n",
    "#plot_model(gan.discriminator, to_file= datapath + 'fcn-disc-arch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDPRcRE9Y2sk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXdoCbsfY2zI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2801
    },
    "colab_type": "code",
    "id": "rzgPsT4Cqywo",
    "outputId": "b534706e-6697-4b52-c3d8-ea0717500ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 512)               1056768   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,188,353\n",
      "Trainable params: 1,188,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 512)          131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 100, 512)          2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 100, 3)            1539      \n",
      "=================================================================\n",
      "Total params: 399,363\n",
      "Trainable params: 398,339\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.693582, acc.: 22.27%] [G loss: 0.688059]\n",
      "1 [D loss: 0.693943, acc.: 67.19%] [G loss: 0.690330]\n",
      "2 [D loss: 0.686971, acc.: 71.48%] [G loss: 0.667248]\n",
      "3 [D loss: 0.687251, acc.: 72.27%] [G loss: 0.693699]\n",
      "4 [D loss: 0.686737, acc.: 70.31%] [G loss: 0.696602]\n",
      "5 [D loss: 0.666245, acc.: 73.83%] [G loss: 0.684420]\n",
      "6 [D loss: 0.669571, acc.: 67.97%] [G loss: 0.674589]\n",
      "7 [D loss: 0.670496, acc.: 70.70%] [G loss: 0.699576]\n",
      "8 [D loss: 0.688478, acc.: 68.75%] [G loss: 0.664483]\n",
      "9 [D loss: 0.719150, acc.: 60.94%] [G loss: 0.649559]\n",
      "10 [D loss: 0.749386, acc.: 58.20%] [G loss: 0.653076]\n",
      "11 [D loss: 0.796250, acc.: 54.30%] [G loss: 0.637054]\n",
      "12 [D loss: 0.862844, acc.: 50.39%] [G loss: 0.599177]\n",
      "13 [D loss: 1.035884, acc.: 50.00%] [G loss: 0.540412]\n",
      "14 [D loss: 1.260517, acc.: 50.00%] [G loss: 0.366108]\n",
      "15 [D loss: 1.108932, acc.: 53.12%] [G loss: 0.321867]\n",
      "16 [D loss: 0.924398, acc.: 59.38%] [G loss: 0.419933]\n",
      "17 [D loss: 0.811818, acc.: 63.28%] [G loss: 0.498268]\n",
      "18 [D loss: 0.717024, acc.: 73.83%] [G loss: 0.606499]\n",
      "19 [D loss: 0.647458, acc.: 82.42%] [G loss: 0.653972]\n",
      "20 [D loss: 0.654220, acc.: 76.95%] [G loss: 0.687324]\n",
      "21 [D loss: 0.691028, acc.: 72.66%] [G loss: 0.734743]\n",
      "22 [D loss: 0.632699, acc.: 77.34%] [G loss: 0.814418]\n",
      "23 [D loss: 0.637506, acc.: 63.28%] [G loss: 0.829486]\n",
      "24 [D loss: 0.635145, acc.: 67.19%] [G loss: 0.868389]\n",
      "25 [D loss: 0.609541, acc.: 69.14%] [G loss: 0.926961]\n",
      "26 [D loss: 0.603138, acc.: 75.00%] [G loss: 0.932061]\n",
      "27 [D loss: 0.574278, acc.: 77.34%] [G loss: 0.946016]\n",
      "28 [D loss: 0.613150, acc.: 75.78%] [G loss: 0.926819]\n",
      "29 [D loss: 0.645675, acc.: 77.73%] [G loss: 0.899377]\n",
      "30 [D loss: 0.678084, acc.: 75.00%] [G loss: 0.874487]\n",
      "31 [D loss: 0.717462, acc.: 74.22%] [G loss: 0.830573]\n",
      "32 [D loss: 0.679948, acc.: 76.17%] [G loss: 0.821118]\n",
      "33 [D loss: 0.667833, acc.: 79.69%] [G loss: 0.776002]\n",
      "34 [D loss: 0.591312, acc.: 86.72%] [G loss: 0.769164]\n",
      "35 [D loss: 0.618503, acc.: 84.38%] [G loss: 0.751387]\n",
      "36 [D loss: 0.601976, acc.: 84.77%] [G loss: 0.767115]\n",
      "37 [D loss: 0.612785, acc.: 84.38%] [G loss: 0.736880]\n",
      "38 [D loss: 0.612206, acc.: 83.59%] [G loss: 0.725911]\n",
      "39 [D loss: 0.568963, acc.: 89.06%] [G loss: 0.732727]\n",
      "40 [D loss: 0.541394, acc.: 92.97%] [G loss: 0.751155]\n",
      "41 [D loss: 0.478387, acc.: 96.88%] [G loss: 0.754589]\n",
      "42 [D loss: 0.431641, acc.: 97.66%] [G loss: 0.715058]\n",
      "43 [D loss: 0.398718, acc.: 100.00%] [G loss: 0.730190]\n",
      "44 [D loss: 0.382121, acc.: 100.00%] [G loss: 0.693271]\n",
      "45 [D loss: 0.379780, acc.: 100.00%] [G loss: 0.689468]\n",
      "46 [D loss: 0.379323, acc.: 100.00%] [G loss: 0.670142]\n",
      "47 [D loss: 0.372526, acc.: 100.00%] [G loss: 0.666197]\n",
      "48 [D loss: 0.380977, acc.: 100.00%] [G loss: 0.649612]\n",
      "49 [D loss: 0.385372, acc.: 100.00%] [G loss: 0.641147]\n",
      "50 [D loss: 0.386898, acc.: 100.00%] [G loss: 0.616478]\n",
      "51 [D loss: 0.376042, acc.: 100.00%] [G loss: 0.613694]\n",
      "52 [D loss: 0.380856, acc.: 100.00%] [G loss: 0.600855]\n",
      "53 [D loss: 0.372563, acc.: 100.00%] [G loss: 0.616517]\n",
      "54 [D loss: 0.367600, acc.: 100.00%] [G loss: 0.627170]\n",
      "55 [D loss: 0.361355, acc.: 100.00%] [G loss: 0.618120]\n",
      "56 [D loss: 0.361603, acc.: 100.00%] [G loss: 0.611789]\n",
      "57 [D loss: 0.365109, acc.: 100.00%] [G loss: 0.588437]\n",
      "58 [D loss: 0.363570, acc.: 100.00%] [G loss: 0.619792]\n",
      "59 [D loss: 0.359490, acc.: 100.00%] [G loss: 0.614394]\n",
      "60 [D loss: 0.365789, acc.: 100.00%] [G loss: 0.626040]\n",
      "61 [D loss: 0.359256, acc.: 100.00%] [G loss: 0.645056]\n",
      "62 [D loss: 0.349956, acc.: 100.00%] [G loss: 0.623517]\n",
      "63 [D loss: 0.338394, acc.: 100.00%] [G loss: 0.599936]\n",
      "64 [D loss: 0.314216, acc.: 100.00%] [G loss: 0.537995]\n",
      "65 [D loss: 0.292505, acc.: 100.00%] [G loss: 0.499258]\n",
      "66 [D loss: 0.269707, acc.: 100.00%] [G loss: 0.436376]\n",
      "67 [D loss: 0.244059, acc.: 100.00%] [G loss: 0.342386]\n",
      "68 [D loss: 0.214512, acc.: 100.00%] [G loss: 0.276679]\n",
      "69 [D loss: 0.177598, acc.: 100.00%] [G loss: 0.183422]\n",
      "70 [D loss: 0.141363, acc.: 100.00%] [G loss: 0.129153]\n",
      "71 [D loss: 0.098206, acc.: 100.00%] [G loss: 0.090036]\n",
      "72 [D loss: 0.063918, acc.: 100.00%] [G loss: 0.055460]\n",
      "73 [D loss: 0.039565, acc.: 100.00%] [G loss: 0.041868]\n",
      "74 [D loss: 0.028689, acc.: 100.00%] [G loss: 0.071437]\n",
      "75 [D loss: 0.025800, acc.: 100.00%] [G loss: 0.299410]\n",
      "76 [D loss: 1.149634, acc.: 50.00%] [G loss: 0.350502]\n",
      "77 [D loss: 0.329632, acc.: 79.30%] [G loss: 0.107967]\n",
      "78 [D loss: 0.092071, acc.: 100.00%] [G loss: 0.095440]\n",
      "79 [D loss: 0.053931, acc.: 100.00%] [G loss: 0.082796]\n",
      "80 [D loss: 0.044762, acc.: 100.00%] [G loss: 0.112619]\n",
      "81 [D loss: 0.038252, acc.: 100.00%] [G loss: 0.181904]\n",
      "82 [D loss: 0.038005, acc.: 100.00%] [G loss: 0.301922]\n",
      "83 [D loss: 0.035314, acc.: 100.00%] [G loss: 0.468692]\n",
      "84 [D loss: 0.030291, acc.: 100.00%] [G loss: 0.616863]\n",
      "85 [D loss: 0.032534, acc.: 100.00%] [G loss: 0.670854]\n",
      "86 [D loss: 0.029654, acc.: 100.00%] [G loss: 0.780452]\n",
      "87 [D loss: 0.025845, acc.: 100.00%] [G loss: 0.721229]\n",
      "88 [D loss: 0.023852, acc.: 100.00%] [G loss: 0.594230]\n",
      "89 [D loss: 0.027469, acc.: 100.00%] [G loss: 0.503738]\n",
      "90 [D loss: 0.037316, acc.: 100.00%] [G loss: 0.483432]\n",
      "91 [D loss: 0.045576, acc.: 100.00%] [G loss: 0.466247]\n",
      "92 [D loss: 0.060245, acc.: 100.00%] [G loss: 0.761310]\n",
      "93 [D loss: 0.048146, acc.: 100.00%] [G loss: 1.389655]\n",
      "94 [D loss: 0.052863, acc.: 100.00%] [G loss: 2.313201]\n",
      "95 [D loss: 0.042472, acc.: 100.00%] [G loss: 3.077765]\n",
      "96 [D loss: 0.035912, acc.: 100.00%] [G loss: 3.629203]\n",
      "97 [D loss: 0.027786, acc.: 100.00%] [G loss: 3.938418]\n",
      "98 [D loss: 0.022388, acc.: 100.00%] [G loss: 4.178377]\n",
      "99 [D loss: 0.016054, acc.: 100.00%] [G loss: 4.454443]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_crnn = GAN(rows=100)\n",
    "gan_crnn.train(epochs=100, batch_size=128, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1363
    },
    "colab_type": "code",
    "id": "ImiKg7HBAJR4",
    "outputId": "9c0464be-a01a-4ab8-9d53-314552260ff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22265625,\n",
       " 0.671875,\n",
       " 0.71484375,\n",
       " 0.72265625,\n",
       " 0.703125,\n",
       " 0.73828125,\n",
       " 0.6796875,\n",
       " 0.70703125,\n",
       " 0.6875,\n",
       " 0.609375,\n",
       " 0.58203125,\n",
       " 0.54296875,\n",
       " 0.50390625,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.53125,\n",
       " 0.59375,\n",
       " 0.6328125,\n",
       " 0.73828125,\n",
       " 0.82421875,\n",
       " 0.76953125,\n",
       " 0.7265625,\n",
       " 0.7734375,\n",
       " 0.6328125,\n",
       " 0.671875,\n",
       " 0.69140625,\n",
       " 0.75,\n",
       " 0.7734375,\n",
       " 0.7578125,\n",
       " 0.77734375,\n",
       " 0.75,\n",
       " 0.7421875,\n",
       " 0.76171875,\n",
       " 0.796875,\n",
       " 0.8671875,\n",
       " 0.84375,\n",
       " 0.84765625,\n",
       " 0.84375,\n",
       " 0.8359375,\n",
       " 0.890625,\n",
       " 0.9296875,\n",
       " 0.96875,\n",
       " 0.9765625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg = gan_crnn.disc_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CkC6_x28v34"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(gan_crnn.disc_loss_real[:75], c='dodgerblue')\n",
    "#plt.plot(gan_crnn.disc_loss_fake[:74], c='orange')\n",
    "plt.plot(gan_crnn.gen_loss[:75], c='green')\n",
    "plt.title(\"GAN Loss per Epoch\")\n",
    "plt.legend(['Discriminator', 'Generator'], loc=\"best\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(datapath + 'loss_epoch_lstm.png', transparent=True)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MB4vZnOcwtPS"
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (1, 100, 1))\n",
    "gen_seqs = gan_crnn.generator.predict(noise)\n",
    "gen_seqs = gen_seqs.reshape(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uflrptWyyANh"
   },
   "outputs": [],
   "source": [
    "gen_seq_denorm = np.zeros(gen_seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPuoCXdFxdge"
   },
   "outputs": [],
   "source": [
    "gen_seq_denorm[:, 0] = (gen_seqs[:, 0] * (freq_max - freq_min)) + freq_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRWHZ2AhyIcO"
   },
   "outputs": [],
   "source": [
    "gen_seq_denorm[:, 1] = (gen_seqs[:, 1] * 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2hUCYnTyOc1"
   },
   "outputs": [],
   "source": [
    "gen_seq_denorm[:, 2] = (gen_seqs[:, 2] * (time_max - time_min)) + time_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8QXwAwxw0w0b",
    "outputId": "07942f8e-b9bf-4c29-8fae-ce3f0c133f5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gen_seqs[:, 2] < -1).any() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "_Xmt_nVPzI9I",
    "outputId": "4dea5c4b-e0d2-4aef-887d-be7df83306aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log2\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in less\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in less\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = get_scale_consistency(gen_seq_denorm[:, 0], 13, 107)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g4S0FAqh6sNv",
    "outputId": "ce314317-d562-4d89-cd11-587124ae047c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 13}"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetitions(a.astype(int), rep_min=1, rep_max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "85m2Ayr07dHe",
    "outputId": "29212576-3e38-43c9-fdaf-28a44304a6aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tone_span(a.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m-iAl-A37gVt",
    "outputId": "188ad12c-745e-4659-9c94-6b91df44e7d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tone_var(a.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "dios1o1U76RH",
    "outputId": "97f62e87-552a-4848-b09e-f1d966c640b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log2\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in less\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in less\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-7e87e87d829c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'note_on'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mmid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'new_song_crnn'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.mid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mido/messages/messages.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, type, **args)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sysex'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmsgdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSysexData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_py2_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcheck_msgdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mido/messages/checks.py\u001b[0m in \u001b[0;36mcheck_msgdict\u001b[0;34m(msgdict)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 '{} message has no attribute {}'.format(spec['type'], name))\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mcheck_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mido/messages/checks.py\u001b[0m in \u001b[0;36mcheck_value\u001b[0;34m(name, value)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0m_CHECKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mido/messages/checks.py\u001b[0m in \u001b[0;36mcheck_data_byte\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data byte must be int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data byte must be in range 0..127'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: data byte must be in range 0..127"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "  \n",
    "  mid = MidiFile()\n",
    "  track = MidiTrack()\n",
    "  mid.tracks.append(track)  \n",
    "  \n",
    "  noise = np.random.normal(0, 1, (1, 100, 1))\n",
    "  gen_seqs = gan_crnn.generator.predict(noise)\n",
    "  gen_seqs = gen_seqs.reshape(100, 3)\n",
    "  gen_seq_denorm = np.zeros(gen_seqs.shape)\n",
    "  gen_seq_denorm[:, 0] = (gen_seqs[:, 0] * (freq_max - freq_min)) + freq_min\n",
    "  a, b = get_scale_consistency(gen_seq_denorm[:, 0], 13, 107)\n",
    "  a = a.astype(int)\n",
    "  gen_seq_denorm[:, 1] = (gen_seqs[:, 1] * 127).astype(int)\n",
    "  gen_seq_denorm[:, 2] = (gen_seqs[:, 2] * (time_max - time_min)) + time_min\n",
    "  \n",
    "  for i in range(0, len(a)):\n",
    "    track.append(Message('note_on', note= a[i], velocity=100, time=200))\n",
    "\n",
    "  mid.save(datapath + 'new_song_crnn' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "flGlJc2vT_XH",
    "outputId": "0074dbdd-d496-4bd9-87bb-d0d852a54722"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gen_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPULuGgag4Sz"
   },
   "source": [
    "# Statistical Evaluation of Generated Music\n",
    "\n",
    "\n",
    "\n",
    "*   **Scale Consistency** :  Fraction of tones that were part of a standard scale.\n",
    "*   **Repetitions**: Count of the number of repeated\n",
    "short subsequences where a score is given based on the\n",
    "amount of recurrence.\n",
    "*  **Tone Span**: Measure of the number of half-\n",
    "tone steps between the lowest and the highest tone in a\n",
    "sample.\n",
    "*  **Tone variation**: Number of different tones in the generated sequence  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQFgGoMkhrPw"
   },
   "outputs": [],
   "source": [
    "def get_scale_consistency(generated_tones_freq, min_midi_no, max_midi_no):\n",
    "  midi_numbers = 12 * np.log2(generated_tones_freq / 440) + 69\n",
    "  unmatched = np.sum(midi_numbers < min_midi_no) + np.sum(midi_numbers > max_midi_no)\n",
    "  midi_numbers_float = midi_numbers - midi_numbers.astype(int)\n",
    "  matched_vals_low = midi_numbers_float < 0.45\n",
    "  matched_vals_high = midi_numbers_float > 0.55\n",
    "  matched_vals = np.logical_or(matched_vals_low, matched_vals_high)\n",
    "  return midi_numbers[matched_vals], (len(midi_numbers[matched_vals])) / len(generated_tones_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qKAWX7RtDZC"
   },
   "outputs": [],
   "source": [
    "def repetitions(matched_tones, rep_min, rep_max):\n",
    "  rs = {}\n",
    "  for l in range(rep_min, rep_max):\n",
    "    rs[l] = 0\n",
    "    for i in range(len(matched_tones)-l*2):\n",
    "      for j in range(i+l,len(matched_tones)-l):\n",
    "        if np.array_equal(matched_tones[i:i+l], matched_tones[j:j+l]):\n",
    "          rs[l] += 1\n",
    "  rs2 = {}\n",
    "  for r in rs:\n",
    "    if rs[r]:\n",
    "      rs2[r] = rs[r]\n",
    "  return rs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHD8r4TptoIn"
   },
   "outputs": [],
   "source": [
    "def tone_span(tones):\n",
    "  return np.max(tones) - np.min(tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDL8fNqNtwC3"
   },
   "outputs": [],
   "source": [
    "def tone_var(tones):\n",
    "  return len(np.unique(tones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5Qg7yyudQ29g",
    "outputId": "cb9bc8c8-30ad-4038-fd6b-d9a311051a16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = get_scale_consistency(gen_seq_denorm[:, 0], 13, 107)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E38Iwj7TRYvc",
    "outputId": "0fb95a45-0725-4ef5-da76-08f705d6bbb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 4, 3: 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetitions(a.astype(int), rep_min=2, rep_max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T9Fu99nfSmYF",
    "outputId": "30283105-96a3-4697-d486-ee095665fc45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tone_span(a.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TmR1sfRzSve1",
    "outputId": "b1d0916b-b29b-4862-bc6f-71e886ffac03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tone_var(a.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSqyLVv3SyCv"
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"model-10.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhfn4yG1Y90_"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "  \n",
    "  mid = MidiFile()\n",
    "  track = MidiTrack()\n",
    "  mid.tracks.append(track)  \n",
    "  b_val = []\n",
    "  tone_span_val = []\n",
    "  tone_var_list = []\n",
    "  rep_list = []\n",
    "  \n",
    "  for gen_seqs in final_seqs:\n",
    "      gen_seqs = gen_seqs.reshape(100, 3)\n",
    "      gen_seq_denorm = np.zeros(gen_seqs.shape)\n",
    "      gen_seq_denorm[:, 0] = (gen_seqs[:, 0] * (freq_max - freq_min)) + freq_min\n",
    "      a, b = get_scale_consistency(gen_seq_denorm[:, 0], 13, 107)\n",
    "      a = a.astype(int)\n",
    "      b_val.append(b)\n",
    "      #print(repetitions(a.astype(int), rep_min=2, rep_max=5))\n",
    "      gen_seq_denorm[:, 1] = (gen_seqs[:, 1] * 127).astype(int)\n",
    "      gen_seq_denorm[:, 2] = (gen_seqs[:, 2] * (time_max - time_min)) + time_min\n",
    "      tone_span_val.append(tone_span(a.astype(int)))\n",
    "      tone_var_list.append(tone_var(a.astype(int)))\n",
    "      rep_list.append(repetitions(a.astype(int), rep_min=2, rep_max=5))\n",
    "      for i in range(0, len(a)):\n",
    "        track.append(Message('note_on', note= a[i], velocity=100, time=200))\n",
    "\n",
    "      mid.save(datapath + 'new_song' + str(i) + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "HD4-9QGPh44J",
    "outputId": "f54563d7-b62f-4a8a-f304-add4592201a3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmH+EDVQDBOYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_values1(b_val, tone_span_val, tone_var_list):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = plt.subplot(111)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.xticks(range(0, 2500, 200))\n",
    "    y = range(0, 2500, 50)\n",
    "    \n",
    "    #c1 = plt.plot(y, np.squeeze(input_node), color=\"orange\", label=\"input_node\")\n",
    "    #c2 = plt.plot(y, np.squeeze(internal_state), color=\"teal\", label=\"internal_state\")\n",
    "    #c2 = plt.plot(y, np.squeeze(input_gate), color=\"blue\", label=\"input_gate\")\n",
    "    c1 = plt.plot(y, np.squeeze(b_val), color=\"red\", label=\"Scale consistency\", linewidth=2.0)\n",
    "    c2 = plt.plot(y, np.squeeze(tone_span_val), color=\"dodgerblue\", label=\"Tonespan\", linewidth=1.0)\n",
    "    ax.legend()\n",
    "    plt.title(\"Plot\")\n",
    "    #plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "plot_values1(b_val, tone_span_val, tone_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "colab_type": "code",
    "id": "ucL6MQqdgPtE",
    "outputId": "5e32a17a-e72e-4e67-e68a-2ba333ec30a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.87,\n",
       " 0.85,\n",
       " 0.93,\n",
       " 1.0,\n",
       " 0.93,\n",
       " 0.87,\n",
       " 1.0,\n",
       " 0.84,\n",
       " 0.93,\n",
       " 0.87,\n",
       " 0.93,\n",
       " 0.91,\n",
       " 0.81,\n",
       " 0.86,\n",
       " 0.9,\n",
       " 0.92,\n",
       " 0.9,\n",
       " 0.92,\n",
       " 1.0,\n",
       " 0.85,\n",
       " 0.88,\n",
       " 0.89,\n",
       " 0.85,\n",
       " 0.87,\n",
       " 0.94,\n",
       " 0.89,\n",
       " 0.93,\n",
       " 0.89,\n",
       " 0.96,\n",
       " 0.92,\n",
       " 0.96,\n",
       " 0.87,\n",
       " 0.88,\n",
       " 0.89,\n",
       " 0.84,\n",
       " 0.92,\n",
       " 0.83,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.91,\n",
       " 0.91,\n",
       " 0.91,\n",
       " 0.92,\n",
       " 0.88,\n",
       " 0.88,\n",
       " 0.97,\n",
       " 0.97,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.93]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0rnDZUtgPpG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqiNsBdbgPg5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1750
    },
    "colab_type": "code",
    "id": "BZw9I9ApaC_O",
    "outputId": "404e9b08-1404-40e9-a0be-53e1b34d10e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.7998696  0.9530449  0.84937096]\n",
      "  [0.15825713 0.3300386  0.45573384]\n",
      "  [0.6101406  0.2875145  0.7044674 ]\n",
      "  [0.6011729  0.40458614 0.16909906]\n",
      "  [0.8104598  0.76960146 0.28276697]\n",
      "  [0.4895568  0.6299223  0.14528283]\n",
      "  [0.28941417 0.6289892  0.4558926 ]\n",
      "  [0.44848385 0.61190724 0.48667514]\n",
      "  [0.9077398  0.3615174  0.50546217]\n",
      "  [0.16157082 0.6962994  0.69297093]\n",
      "  [0.8122356  0.0809851  0.1408329 ]\n",
      "  [0.19495451 0.42973825 0.16819993]\n",
      "  [0.75539356 0.24683535 0.58806515]\n",
      "  [0.22783676 0.79877037 0.69564563]\n",
      "  [0.65882087 0.6676462  0.9434037 ]\n",
      "  [0.73815566 0.7785399  0.31890368]\n",
      "  [0.2978382  0.9436167  0.15086755]\n",
      "  [0.46249306 0.9011998  0.31297988]\n",
      "  [0.31169719 0.4616465  0.5049339 ]\n",
      "  [0.51182187 0.35532343 0.17128637]\n",
      "  [0.89728934 0.8039422  0.2912392 ]\n",
      "  [0.20464322 0.46452183 0.604581  ]\n",
      "  [0.9063872  0.21232137 0.7800156 ]\n",
      "  [0.67100614 0.08472264 0.06041312]\n",
      "  [0.01634327 0.06280741 0.35441184]\n",
      "  [0.26210845 0.47217837 0.24281037]\n",
      "  [0.0555031  0.2092371  0.28236604]\n",
      "  [0.8608774  0.95099205 0.885343  ]\n",
      "  [0.07191157 0.8571075  0.73767275]\n",
      "  [0.59957063 0.5888581  0.68417686]\n",
      "  [0.21771789 0.0275324  0.42838937]\n",
      "  [0.39661708 0.6451379  0.8093256 ]\n",
      "  [0.7423362  0.39496595 0.6784051 ]\n",
      "  [0.9752954  0.06778264 0.47010347]\n",
      "  [0.56471467 0.10341078 0.2658285 ]\n",
      "  [0.6321893  0.44800496 0.31284964]\n",
      "  [0.87599176 0.28312233 0.09958506]\n",
      "  [0.07664555 0.6813472  0.18175176]\n",
      "  [0.22134319 0.91967696 0.29722115]\n",
      "  [0.9542147  0.5405159  0.45622858]\n",
      "  [0.6101138  0.94109666 0.24248084]\n",
      "  [0.26623875 0.93905616 0.63558555]\n",
      "  [0.21856454 0.4319295  0.13961771]\n",
      "  [0.11868837 0.923329   0.873823  ]\n",
      "  [0.61448437 0.54409134 0.8009398 ]\n",
      "  [0.3522598  0.2988529  0.37540483]\n",
      "  [0.7103683  0.18913999 0.6293063 ]\n",
      "  [0.48026475 0.7398937  0.65507305]\n",
      "  [0.23787138 0.38498762 0.8292044 ]\n",
      "  [0.78117156 0.47956017 0.28925735]\n",
      "  [0.5294567  0.12191328 0.11803651]\n",
      "  [0.8146234  0.38462597 0.7216494 ]\n",
      "  [0.82075316 0.10287896 0.35874158]\n",
      "  [0.5631131  0.18431762 0.21027091]\n",
      "  [0.83459747 0.3925072  0.5145315 ]\n",
      "  [0.60755116 0.86584306 0.03128102]\n",
      "  [0.75629413 0.03252155 0.8864093 ]\n",
      "  [0.25561428 0.93755865 0.28137362]\n",
      "  [0.250522   0.3445782  0.27808678]\n",
      "  [0.76083577 0.46749637 0.08152533]\n",
      "  [0.2412281  0.7288731  0.04291797]\n",
      "  [0.714005   0.36691588 0.6131178 ]\n",
      "  [0.68576735 0.5597272  0.11924255]\n",
      "  [0.16632336 0.3842548  0.9745091 ]\n",
      "  [0.6850236  0.80601656 0.894328  ]\n",
      "  [0.697989   0.6375835  0.3412326 ]\n",
      "  [0.38092792 0.2721942  0.07367334]\n",
      "  [0.75267273 0.70220464 0.29550666]\n",
      "  [0.78169745 0.3197959  0.84261346]\n",
      "  [0.7141134  0.17033786 0.2869956 ]\n",
      "  [0.8687045  0.4555106  0.12937695]\n",
      "  [0.816585   0.4880411  0.18347752]\n",
      "  [0.5018715  0.19753003 0.9212837 ]\n",
      "  [0.57118315 0.55767584 0.29018688]\n",
      "  [0.6852884  0.8674176  0.554409  ]\n",
      "  [0.20959175 0.80041385 0.80053604]\n",
      "  [0.72122884 0.09933272 0.2140854 ]\n",
      "  [0.82507837 0.5325516  0.8940685 ]\n",
      "  [0.51294607 0.46844292 0.73613375]\n",
      "  [0.03771088 0.28219455 0.4068356 ]\n",
      "  [0.85508424 0.4121113  0.23507735]\n",
      "  [0.17404336 0.88046825 0.55696   ]\n",
      "  [0.9326397  0.35928705 0.7578057 ]\n",
      "  [0.5747838  0.2671802  0.40792242]\n",
      "  [0.27769744 0.89060545 0.5270208 ]\n",
      "  [0.85294735 0.77818054 0.42468265]\n",
      "  [0.74835455 0.6126293  0.5147531 ]\n",
      "  [0.43703154 0.08146048 0.9079064 ]\n",
      "  [0.25046962 0.6797163  0.05773574]\n",
      "  [0.19842052 0.4845575  0.6062155 ]\n",
      "  [0.92161953 0.3429224  0.27948675]\n",
      "  [0.4524691  0.7349769  0.24721605]\n",
      "  [0.6254481  0.2994781  0.89167017]\n",
      "  [0.3640377  0.7141713  0.20929775]\n",
      "  [0.5758841  0.48722786 0.6025478 ]\n",
      "  [0.8896727  0.18347135 0.42837   ]\n",
      "  [0.5464748  0.5868371  0.14652422]\n",
      "  [0.57541984 0.150412   0.39293924]\n",
      "  [0.5663477  0.3067438  0.62284195]\n",
      "  [0.0725593  0.60708576 0.23944646]]]\n"
     ]
    }
   ],
   "source": [
    "for i in final_seqs:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMVDpEliaJ7u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Music Generation GAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
